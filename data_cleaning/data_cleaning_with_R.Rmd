---
title: "Cleaning data with R"
author: "Colin Smith"
date: "February 5, 2018"
output: html_document
---

<div align="left">
<img src="cropped-edi-logo-svg.png" width=150>
</div>


## Introduction

Cleaning data for publication can be a time-consuming task, yet there are multiple software tools available to help. Which tool to use depends on the nature of the data and your level of expertise operating it. Here we demonstrate how to clean data programmatically with R. This approach is good because it:

1. Facilitates cleaning of large datasets
2. Removes human error from the data cleaning process
3. Streamlines cleaning of many datasets of the same format
4. Documents the exact steps and methods applied in the cleaning process, so others can assess whether the data are fit for their use. 

Below is an exercise that presents you with a messy dataset containing several issues commonly found in raw data, and step by step instructions to resolve these issues using R.


#### R packages for cleaning data

There are many R packages and functions useful for cleaning data. We will use `tidyverse`, `lubridate`, and `dataMaid`. `tidyverse` simplifies data imports to RStudio and is good for reformatting data. `lubridate` helps with dates, while `dataMaid` is useful for identifying issues within data columns. We'll use several other functions of base R to solve the data cleaning challenges.


#### The messy dataset

For this exercise we've created a messy dataset containing many common issues encountered in the data cleaning process. These issues include:

* 'Wide' tables that need to be reformatted 'long'.
* 'Long' tables that need to be reformatted 'wide'.
* Relational tables with non-unique keys.
* Duplicate observations (not replicate observations).
* Empty white spaces.
* Use of multiple missing value codes within a single column.
* Presence of characters in numeric columns.
* Inconsistent use of datetime formats within a single column.
* Inconsistent spelling of categorical variables.
* Altered value precision.


## The exercise

It's good to have a strategy for the data cleaning process so you don't get lost in it. An example strategy is:

1. Create a new workspace for your data cleaning project.
2. Gather your data cleaning tools.
3. Get the data and any associated metadata
4. Read the metadata and look at the data to understand what you are working with.
5. Devise a step-by-step plan for the cleaning procedures to be applied.
6. Clean the data while scripting the process.
7. Export the cleaned data to a new file.
8. Save the cleaning script.


#### Step 1: Get the messy data and associated metadata

Create a working directory for this exercise on you computer. [Download the messey dataset from EDI's GitHub](https://github.com/EDIorg/tutorials/tree/master/data_cleaning). Right click and 'save as' each table ('Table_A.csv', 'Table_B.csv', 'Table_C.csv', 'Table_D.csv') to your working directory.

Tables A, B, and C contain daily averaged meteorological data for multiple sites logged during December, January, and February 2018, respectively. Table D contains site characteristics.

#### Step 2: Install and load R packages

Open RStudio. If you don't have R and RStudio installed, [download R and install from here](https://cran.r-project.org/mirrors.html) and [download RStudio and install from here](https://www.rstudio.com/products/rstudio/download/).

We will be using `tidyverse`, `lubridate`, `dataMaid` and some base R functions for our data cleaning exercise.

```{r eval=FALSE}
# Install packages
install.packages(c("tidyverse", "lubridate", "dataMaid"))
```

```{r eval=TRUE, message=FALSE}
# Load packages
library("tidyverse")
library("lubridate")
library("dataMaid")

```


#### Step 3: Read data, view structure, and devise plan

Let's take a closer look at the data before deciding on a set of cleaning operations to implement.

```{r eval=TRUE}
# Set the working directory.
# Substitute the path below with that of your working directory.
setwd("/Users/csmith/Documents/EDI/tutorials/data_cleaning")
```

```{r eval=TRUE}
# Read table A
# Note: A parsing message reports the data class detected for each column.
ta <- read_csv("Table_A.csv")
```

```{r eval=TRUE}
# Read table B
tb <- read_csv("Table_B.csv")
```

```{r eval=TRUE}
# Read table C
tc <- read_csv("Table_C.csv")
```

```{r eval=TRUE}
# Read table D
td <- read_csv("Table_D.csv")
```

Now that we have the data imported into RStudio we can take a look at it's structure.

```{r eval=TRUE}
# View structure of table A
glimpse(ta)

```

```{r eval=TRUE}
# View structure of table B
glimpse(tb)

```

```{r eval=TRUE}
# View structure of table C
glimpse(tc)
                 
```

```{r eval=TRUE}
# View structure of table D
glimpse(td)
                 
```

Tables A, B, and C contain virtually the same information (daily averaged meteorological measurements separated by month) while Table D contains the descriptions of sites listed in tables A, B, and C. For cleaning and archiving these data, it would be best to aggregate tables A, B, and C into a single table (table 1) and leave table D as a separate relational table. Note the inconsistent data classes reported for tables A, B, and C. This suggests issues within columns of the same variable, we will look deeper into the source of these issues once the tables have been aggregated. While it is fine archive these tables in wide format, we may want to accommodate additional measurement variables in this data set while not changing the table structure (i.e. new measurement variables will require new columns). By archiving these data in a consistent format, future updates to this dataset will be simplified, and use of these data in scripted workflows will ensure these workflows will not fail.

Now let's devise a plan for cleaning these data and prepare them for archiving.

* Aggregate tables A, B, and C into a single wide table (table 1)
    * Rename columns of table A to match tables B and C.
    * Add a flag column to table A.
    * Convert date column of table C to be of date class. All date columns must be recognized as date class before the aggregation step otherwise dates will be inaccurately presented.
    * Aggregate tables.
* Check tables 1 and 2 for these common issues:
    * Duplicate observations (not replicate observations). These rows should be found and deleted.
    * Presence of white space leading/trailing values. These white spaces should be removed.
    * Use of multiple missing value codes in a single column. Identify the location of these codes and replace them all with a single missing value code.
    * Characters present in otherwise numerical columns. Find these characters and replace with a missing value code.
    * Case issues. Categorical variables are composed of inconsistent cases. Identify the location of these inconsistencies and replace with a single value.
    * False categorical variables. Sparse categorical variables may not be valid.
    * Duplicate keys. Look in the relational table for duplicate keys and delete any found.
    * Loss of precision. Double check the precision of numeric fields and make any adjustments necessary to ensure precision is accurately represented in the cleaned dataset.
* Reformat table 1 from wide to long.
* Export the cleaned data to new .csv files.
* Store the cleaned dataset with the processing script.


#### Step 4: Aggregate tables

```{r eval=TRUE}
# Before aggregation the date column of the sub-tables must be of date class. Note the date column of table C is of character class. Convert this column to date class using the mdy function (since these dates are in month-day-year format)
tc$date <- mdy(tc$date)
tc

# Rename columns of table A to match tables B and C
colnames(ta) <- c("id", "date", "temp", "rel_hum", "par")

# Add a flag column to table A composed entirely of character class NA.
ta$flag <- rep(NA_character_, nrow(ta))
ta

# Aggregate tables A, B, and C
t1 <- rbind(ta, tb, tc)
t1

```

#### Step 5: Duplicate observations

```{r eval=TRUE}
# Identify row numbers of duplicate observations in table 1
which(duplicated(t1), arr.ind = T)

# Remove these duplicate observations
t1 <- unique.data.frame(t1)
t1

```

#### Setp 6: White spaces

```{r eval=TRUE}
# Remove leading and trailing white spaces around values
t1 <- as_tibble(lapply(t1, trimws))

```

#### Setp 7: Missing value codes

```{r eval=TRUE}
# Check for use of multiple missing value codes in a single column
result <- lapply(t1, identifyMissing)
result

# Yep. Multiple missing value codes used in the temp and rel_hum columns. These can be replaced manually by indexing and reassigning a consistent code.
t1$temp[t1$temp == "-99999"] <- NA
t1$temp[t1$temp == "."] <- NA
t1$rel_hum[t1$rel_hum == "-"] <- NA
t1$rel_hum[t1$rel_hum == "."] <- NA
t1$rel_hum[t1$rel_hum == "NaN"] <- NA

# Re-applying the check ensures we have solved this issue
result <- lapply(t1, identifyMissing)
result

# Alternatively we could use a more automated approach to unifying missing value codes.
replacement_code <- NA
for (i in 1:length(result)){
  if (isTRUE(result[[i]]$problem)){
    problem_vals <- result[[i]]$problemValues
    for (j in 1:length(problem_vals)){
      index <- t1[ ,i] == problem_vals[j]
      index[is.na(index)] <- TRUE
      t1[index, i] <- replacement_code
    }
  }
}


```

#### Setp 8: Characters in numeric columns

Now that we have unified all the missing value codes in the numeric columns of table 1, we can convert any stray characters in these columns to NA using `as.numeric`. This requires some knowledge about which columns should be numeric. Consult the datasets metadata if available or use your best judgement. If application of `as.numeric` results in the majority of values changing to NA, then it is likely that you are dealing with a character column which shouldn't be coerced to numeric.


```{r eval=TRUE}
# par should be a numeric column in table 1 but it's listed as character. There must be characters in this column. Identify these characters and corresponding row numbers.
t1$par[is.na(as.numeric(t1$par))] # characters
which(is.na(as.numeric(t1$par)) == TRUE) # row numbers

# Yep, "sensor fail" and "dog ate this data point" definitely don't belong here. Let's coerce all characters of what should be numeric columns into NA. The numeric columns of table 1 should be: temp, rel_hum, and par.
t1$temp <- as.numeric(t1$temp)
t1$rel_hum <- as.numeric(t1$rel_hum)
t1$par <- as.numeric(t1$par)

```

#### Setp 9: Case issues

```{r eval=TRUE}
# Categorical variables are case sensitive. Search for case issues in columns containing categorical variables.
identifyCaseIssues(t1$id)
identifyCaseIssues(t1$flag)

# There appears to be issues with the id column. We can fix these by making all categorical variables of the id column lower case.
t1$id <- tolower(t1$id)

```


#### Setp 10: False categorical variables

```{r eval=TRUE}
# Some categorical variables may be misspelled. Search for these instances.
identifyLoners(t1$id)

# These are misspellings of "site_1". Replace these false categorical variables.
t1$id[t1$id == "seyete_1"] <- "site_1"
t1$id[t1$id == "site_one"] <- "site_1"
identifyLoners(t1$id)

```

#### Step 11: Duplicate keys

```{r eval=TRUE}
# Relational tables should not contain duplicate keys. Look in the relational table (table D) for duplicate keys and delete any found.
index <- duplicated(td$site_id)
index

# The 4th row contains a duplicate key for site 3. Remove this row.
td <- td[!index, ]
td
```

#### Setp 12: Precision

```{r eval=TRUE}
# Double check the precision of numeric fields and make any adjustments necessary to ensure precision is accurately represented in the cleaned dataset. Precision of table 1 columns should be 2, 1, and 0 decimal places for temp, rel_hum, and par, respectively. Precision of table D should be 0, 0, 3, 3 for elevation, geo_extent_bounding_box, latitude, and longitude, respectively.

# Set precision for table 1
t1$temp <- round(t1$temp, digits = 2)
t1$rel_hum <- round(t1$rel_hum, digits = 1)
t1$par <- round(t1$par, digits = 0)

# Set precision for table D
td$elevation <- round(td$elevation, digits = 0)
td$geo_extent_bounding_box <- round(td$geo_extent_bounding_box, digits = 0)
td$latitude <- round(td$latitude, digits = 3)
td$longitude <- round(td$longitude, digits = 3)


```

#### Setp 13: Reformat long

```{r eval=TRUE}
# Now that we have completed the suite of checks on the variables (columns) of table 1, we will convert this table from wide to long to accomodate possible changes to the variable suite measured at each site.
t1 <- gather(t1, key = "variable", value = "value", temp, rel_hum, par, flag)
t1

```

#### Setp 14: Write to file

```{r eval=TRUE}
# Export the cleaned data to new .csv files. This preserves your raw data tables.
write_csv(t1, "Table_1.csv")
write_csv(td, "Table_2.csv")
```

#### Step 15: Store your clean data and processing script together

Now that you've gone through all this effort to clean your data for publication, it is good practice to store your processing script with your clean data. You will want to archive both of these so you and others can understand the methods and transformations applied to these data. Tracking this chain of events is what we call data provenance.

## Resources

Some helpful resources for quickly recalling functions of the packages we used in this exercise, and much more:

[Data import]()
[Data transformation]()
[Data maid]()

